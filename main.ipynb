{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Welcome to ANDI!\n",
    "This notebook will guide you throughout our project! Let's start by setting up our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting imageio~=2.16.0\n",
      "  Using cached imageio-2.16.2-py3-none-any.whl (3.3 MB)\n",
      "Collecting opencv-python~=4.5.5.62\n",
      "  Using cached opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n",
      "Collecting numpy~=1.22.2\n",
      "  Using cached numpy-1.22.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Collecting pandas~=1.4.1\n",
      "  Using cached pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "Collecting tqdm~=4.63.0\n",
      "  Using cached tqdm-4.63.2-py2.py3-none-any.whl (76 kB)\n",
      "Collecting argparse~=1.4.0\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting matplotlib~=3.5.1\n",
      "  Using cached matplotlib-3.5.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
      "Collecting scikit-learn~=1.0.2\n",
      "  Using cached scikit_learn-1.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "Collecting Pillow~=9.0.1\n",
      "  Using cached Pillow-9.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Collecting PyYAML~=6.0\n",
      "  Using cached PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "Collecting wandb~=0.12.11\n",
      "  Using cached wandb-0.12.15-py2.py3-none-any.whl (1.8 MB)\n",
      "Collecting utils~=1.0.1\n",
      "  Using cached utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
      "Collecting seaborn~=0.11.2\n",
      "  Using cached seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "Collecting requests~=2.27.1\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting thop~=0.0.31.post2005241907\n",
      "  Using cached thop-0.0.31.post2005241907-py3-none-any.whl (8.7 kB)\n",
      "Collecting pycocotools~=2.0.4\n",
      "  Using cached pycocotools-2.0.4-cp38-cp38-linux_x86_64.whl\n",
      "Collecting plotly~=5.6.0\n",
      "  Using cached plotly-5.6.0-py2.py3-none-any.whl (27.7 MB)\n",
      "Collecting torch==1.11.0+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp38-cp38-linux_x86_64.whl (1637.0 MB)\n",
      "\u001B[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 435.5 MB 1.9 MB/s eta 0:10:407"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt --find-links https://download.pytorch.org/whl/torch_stable.html\n",
    "#if you have conda\n",
    "#conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?export=download&id=1YXlG77XbMA0a4d12qcUSLa9sMp4rk6Nx\n",
      "To: /mnt/77A80709148517D5/IFT6759_Deep_learning/data/unseen_data_test.zip\n",
      "\n",
      "  0%|          | 0.00/24.4M [00:00<?, ?B/s]\u001B[A\n",
      "  2%|â–         | 524k/24.4M [00:00<00:13, 1.78MB/s]\u001B[A\n",
      " 11%|â–ˆ         | 2.62M/24.4M [00:00<00:02, 7.51MB/s]\u001B[A\n",
      " 19%|â–ˆâ–‰        | 4.72M/24.4M [00:00<00:02, 8.10MB/s]\u001B[A\n",
      " 28%|â–ˆâ–ˆâ–Š       | 6.82M/24.4M [00:00<00:02, 7.15MB/s]\u001B[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–Š      | 9.44M/24.4M [00:01<00:01, 10.1MB/s]\u001B[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 11.5M/24.4M [00:01<00:01, 9.57MB/s]\u001B[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 13.6M/24.4M [00:01<00:00, 11.6MB/s]\u001B[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 15.7M/24.4M [00:01<00:00, 10.6MB/s]\u001B[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 18.4M/24.4M [00:01<00:00, 13.0MB/s]\u001B[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 19.9M/24.4M [00:02<00:00, 10.4MB/s]\u001B[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 21.5M/24.4M [00:02<00:00, 11.3MB/s]\u001B[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.4M/24.4M [00:02<00:00, 9.76MB/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 txt file have been deleted.\n",
      "Data is ready for training!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gdown\n",
    "import shutil\n",
    "import os\n",
    "from data.delete_empty import DeleteEmpty\n",
    "delete=DeleteEmpty()\n",
    "\n",
    "if not os.path.exists(\"data/data_split2\") :\n",
    "    if not os.path.exists(\"data/data_split2.zip\") :\n",
    "        gdown.download(\"https://drive.google.com/uc?export=download&id=1tKAhkas5sCnIYurURylS63ZgBTdsrg_Q\",\"data/data_split2.zip\")\n",
    "    shutil.unpack_archive(\"data/data_split2.zip\", \"data\")\n",
    "\n",
    "if not os.path.exists(\"data/unseen_data_test\") :\n",
    "    gdown.download(\"https://drive.google.com/uc?export=download&id=1YXlG77XbMA0a4d12qcUSLa9sMp4rk6Nx\",\"data/unseen_data_test.zip\")\n",
    "    shutil.unpack_archive(\"data/unseen_data_test.zip\",\"data\")\n",
    "\n",
    "os.remove(\"data/unseen_data_test.zip\")\n",
    "delete.delete()\n",
    "#lets keep data_split2.zip just in case...\n",
    "print(\"Data is ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# This section is for training. If you already trained a model, you can skip ahead to the next section\n",
    "\n",
    "Training a model is easy! To see the available options :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "! python run_train.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Where the dataset corresponds to the different experiences as describe in our report.\n",
    "\n",
    "Let's try something simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sh: 1: source: not found\r\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mweights=models/yolov5/yolov5s.pt, cfg=, data=/mnt/77A80709148517D5/IFT6759_Deep_learning/data/data_split2/data_split2.yaml, hyp=models/yolov5/data/hyps/hyp.scratch-low.yaml, epochs=20, batch_size=8, imgsz=320, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=models/yolov5/runs/train, name=exp, exist_ok=True, quad=False, cos_lr=False, label_smoothing=0.0, patience=5, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\n",
      "\u001B[34m\u001B[1mgithub: \u001B[0mskipping check (not a git repository), for updates see https://github.com/ultralytics/yolov5\r\n",
      "YOLOv5 ðŸš€ 2997acc torch 1.11.0+cu113 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5945MiB)\r\n",
      "\r\n",
      "\u001B[34m\u001B[1mhyperparameters: \u001B[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\n",
      "\u001B[34m\u001B[1mWeights & Biases: \u001B[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\r\n",
      "\u001B[34m\u001B[1mTensorBoard: \u001B[0mStart with 'tensorboard --logdir models/yolov5/runs/train', view at http://localhost:6006/\r\n",
      "Overriding model.yaml nc=80 with nc=14\r\n",
      "\r\n",
      "                 from  n    params  module                                  arguments                     \r\n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \r\n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \r\n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \r\n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \r\n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \r\n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \r\n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \r\n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \r\n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \r\n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \r\n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \r\n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \r\n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \r\n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \r\n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \r\n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \r\n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \r\n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \r\n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \r\n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \r\n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \r\n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \r\n",
      " 24      [17, 20, 23]  1     51243  models.yolo.Detect                      [14, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\r\n",
      "Model summary: 270 layers, 7057387 parameters, 7057387 gradients\r\n",
      "\r\n",
      "Transferred 343/349 items from models/yolov5/yolov5s.pt\r\n",
      "Scaled weight_decay = 0.0005\r\n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\r\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning '/mnt/77A80709148517D5/IFT6759_Deep_learning/data/data_split2/tr\u001B[0m\r\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning '/mnt/77A80709148517D5/IFT6759_Deep_learning/data/data_split2/vali\u001B[0m\r\n",
      "Plotting labels to models/yolov5/runs/train/exp/labels.jpg... \r\n",
      "\r\n",
      "\u001B[34m\u001B[1mAutoAnchor: \u001B[0m5.65 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\r\n",
      "Image sizes 320 train, 320 val\r\n",
      "Using 8 dataloader workers\r\n",
      "Logging results to \u001B[1mmodels/yolov5/runs/train/exp\u001B[0m\r\n",
      "Starting training for 20 epochs...\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "      0/19    0.872G    0.0813   0.01649   0.06441        15       320:  29%|â–ˆâ–ˆâ–Š^C\r\n",
      "      0/19    0.872G    0.0813   0.01649   0.06441        15       320:  29%|â–ˆâ–ˆâ–Š\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/mnt/77A80709148517D5/IFT6759_Deep_learning/models/yolov5/train.py\", line 650, in <module>\r\n",
      "    main(opt)\r\n",
      "  File \"/mnt/77A80709148517D5/IFT6759_Deep_learning/models/yolov5/train.py\", line 546, in main\r\n",
      "    train(opt.hyp, opt, device, callbacks)\r\n",
      "  File \"/mnt/77A80709148517D5/IFT6759_Deep_learning/models/yolov5/train.py\", line 337, in train\r\n",
      "    pred = model(imgs)  # forward\r\n",
      "  File \"/mnt/77A80709148517D5/IFT6759_Deep_learning/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\r\n",
      "    return forward_call(*input, **kwargs)\r\n",
      "  File \"/mnt/77A80709148517D5/IFT6759_Deep_learning/models/yolov5/models/yolo.py\", line 126, in forward\r\n",
      "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\r\n",
      "  File \"/mnt/77A80709148517D5/IFT6759_Deep_learning/models/yolov5/models/yolo.py\", line 149, in _forward_once\r\n",
      "    x = m(x)  # run\r\n",
      "  File \"/mnt/77A80709148517D5/IFT6759_Deep_learning/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\r\n",
      "    return forward_call(*input, **kwargs)\r\n",
      "  File \"/mnt/77A80709148517D5/IFT6759_Deep_learning/models/yolov5/models/common.py\", line 139, in forward\r\n",
      "    return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))\r\n",
      "  File \"/mnt/77A80709148517D5/IFT6759_Deep_learning/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\r\n",
      "    return forward_call(*input, **kwargs)\r\n",
      "  File \"/mnt/77A80709148517D5/IFT6759_Deep_learning/models/yolov5/models/common.py\", line 47, in forward\r\n",
      "    return self.act(self.bn(self.conv(x)))\r\n",
      "  File \"/mnt/77A80709148517D5/IFT6759_Deep_learning/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\r\n",
      "    return forward_call(*input, **kwargs)\r\n",
      "  File \"/mnt/77A80709148517D5/IFT6759_Deep_learning/venv/lib/python3.10/site-packages/torch/nn/modules/activation.py\", line 394, in forward\r\n",
      "    return F.silu(input, inplace=self.inplace)\r\n",
      "  File \"/mnt/77A80709148517D5/IFT6759_Deep_learning/venv/lib/python3.10/site-packages/torch/nn/functional.py\", line 2031, in silu\r\n",
      "    return torch._C._nn.silu_(input)\r\n",
      "  File \"/usr/lib/python3.10/traceback.py\", line 213, in format_stack\r\n",
      "    return format_list(extract_stack(f, limit=limit))\r\n",
      "  File \"/usr/lib/python3.10/traceback.py\", line 227, in extract_stack\r\n",
      "    stack = StackSummary.extract(walk_stack(f), limit=limit)\r\n",
      "  File \"/usr/lib/python3.10/traceback.py\", line 379, in extract\r\n",
      "    linecache.checkcache(filename)\r\n",
      "  File \"/usr/lib/python3.10/linecache.py\", line 72, in checkcache\r\n",
      "    stat = os.stat(fullname)\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#! python run_train.py --model alexnet --dataset 2 --no-wandb --epoch 20\n",
    "! python run_train.py --model yolo --dataset 2 --no-wandb --epoch 20\n",
    "#! python run_train.py --model alexnet --dataset 4 --no-wandb --epoch 20\n",
    "#! python run_train.py --model yolo --dataset 4 --no-wandb --epoch 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Now lets test our model!\n",
    "\n",
    "To see the usage of the detect function we do :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: detect.py [-h] -t {seen,unseen} -m {alexnet,resnext50_32x4d,vgg19} -d\r\n",
      "                 {1,2,3,4}\r\n",
      "\r\n",
      "Launch testing for a specific model\r\n",
      "\r\n",
      "options:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -t {seen,unseen}, --testset {seen,unseen}\r\n",
      "                        Choice of the test set 1-seen locations 2-unseen\r\n",
      "                        locations\r\n",
      "  -m {alexnet,resnext50_32x4d,vgg19}, --model {alexnet,resnext50_32x4d,vgg19}\r\n",
      "                        Choice of the model\r\n",
      "  -d {1,2,3,4}, --dataset {1,2,3,4}\r\n",
      "                        Version of the training dataset used\r\n"
     ]
    }
   ],
   "source": [
    "! python detect.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now lets continue with our alexnet example (assuming you have done the training!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! python run_detect.py --model yolo --dataset 3 --testset seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}